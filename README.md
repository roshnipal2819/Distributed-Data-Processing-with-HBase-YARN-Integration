# Distributed Data Processing with HBase-YARN Integration

## Overview
This project demonstrates the integration of Apache HBase and YARN for distributed data processing using PySpark.

## Setup
1. Install Java, Hadoop, and HBase.
2. Configure Hadoop and HBase.
3. Install Python dependencies: `pip install -r requirements.txt`
4. Run the data processing script: `python data_processing.py`
5. Run the HBase operations script: `python hbase_operations.py`

## Project Structure
- `data_processing.py`: Data processing with Spark.
- `hbase_operations.py`: HBase interactions.
- `requirements.txt`: Project dependencies.
- `README.md`: Project documentation.

